{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWQ Quantization for dots.ocr Model\n",
    "\n",
    "Using **llm-compressor** (vLLM's official successor to AutoAWQ)\n",
    "\n",
    "**‚ö†Ô∏è BEFORE RUNNING - Verify GPU is enabled:**\n",
    "1. Right panel ‚Üí Settings ‚Üí Accelerator ‚Üí **GPU T4 x2**\n",
    "2. Right panel ‚Üí Settings ‚Üí Internet ‚Üí **On**\n",
    "3. You should see \"GPU T4 x2\" in the top right corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Verify GPU is Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this FIRST to verify GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"\\n‚úì GPU is ready!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå NO GPU DETECTED!\")\n",
    "    print(\"\\nTo fix:\")\n",
    "    print(\"1. Go to Settings (right panel)\")\n",
    "    print(\"2. Under 'Accelerator', select 'GPU T4 x2'\")\n",
    "    print(\"3. Wait for session to restart\")\n",
    "    print(\"4. Re-run this cell\")\n",
    "    raise RuntimeError(\"GPU not available. Enable it in Kaggle settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llmcompressor --quiet\n",
    "!pip install accelerate --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "MODEL_ID = \"rednote-hilab/dots.ocr\"\n",
    "MODEL_DIR = \"/kaggle/working/dots_ocr_original\"\n",
    "OUTPUT_DIR = \"/kaggle/working/dots_ocr_awq_4bit\"\n",
    "\n",
    "print(f\"Downloading {MODEL_ID}...\")\n",
    "snapshot_download(repo_id=MODEL_ID, local_dir=MODEL_DIR, local_dir_use_symlinks=False)\n",
    "print(f\"Downloaded to {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show files\n",
    "for f in os.listdir(MODEL_DIR):\n",
    "    path = os.path.join(MODEL_DIR, f)\n",
    "    if os.path.isfile(path):\n",
    "        print(f\"{f}: {os.path.getsize(path)/1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Model, Tokenizer, and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
    "import gc\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor (includes image processor and video processor)\n",
    "print(\"Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
    "print(f\"Processor loaded: {type(processor).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model on GPU\n",
    "print(\"Loading model to GPU...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",  # Explicitly use CUDA\n",
    ")\n",
    "print(f\"Model loaded! GPU mem: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Calibration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Calibration prompts\n",
    "calibration_texts = [\n",
    "    \"What text is shown in this image?\",\n",
    "    \"Please read and transcribe the text from this document.\",\n",
    "    \"Extract all the text content from the provided image.\",\n",
    "    \"Can you identify and read the text in this picture?\",\n",
    "    \"Transcribe the handwritten text in this image.\",\n",
    "    \"What does this document say?\",\n",
    "    \"Read the printed text from this image.\",\n",
    "    \"Please OCR this image and provide the text.\",\n",
    "    \"Extract text content including numbers and special characters.\",\n",
    "    \"Identify all readable text elements in this image.\",\n",
    "    \"What is written on this page?\",\n",
    "    \"Transcribe the following scanned document.\",\n",
    "    \"Please read the text from this screenshot.\",\n",
    "    \"Extract and format the text content visible in the image.\",\n",
    "    \"Read all text including headers and footnotes.\",\n",
    "    \"What are the words shown in this picture?\",\n",
    "    \"Describe what you see in the image.\",\n",
    "    \"List all words visible in this photograph.\",\n",
    "    \"Convert the printed text to digital format.\",\n",
    "    \"Read and output the document content.\",\n",
    "    \"Extract text from this scanned page.\",\n",
    "    \"What words appear in this image?\",\n",
    "    \"Transcribe the visible text accurately.\",\n",
    "    \"Identify all text elements in the picture.\",\n",
    "    \"Read the content of this document image.\",\n",
    "    \"Extract and list all readable text.\",\n",
    "    \"What is the text content of this image?\",\n",
    "    \"Please digitize the text in this scan.\",\n",
    "    \"Convert image text to editable format.\",\n",
    "    \"Read aloud what this document says.\",\n",
    "    \"Extract every word from this image.\",\n",
    "    \"What textual information is present?\"\n",
    "]\n",
    "\n",
    "calibration_dataset = Dataset.from_dict({\"text\": calibration_texts})\n",
    "print(f\"Calibration dataset: {len(calibration_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize calibration data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = calibration_dataset.map(tokenize_function, batched=True)\n",
    "print(\"Calibration data tokenized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Quantize with AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor.modifiers.quantization import QuantizationModifier\n",
    "from llmcompressor import oneshot\n",
    "\n",
    "# AWQ-style quantization recipe\n",
    "recipe = QuantizationModifier(\n",
    "    targets=\"Linear\",\n",
    "    scheme=\"W4A16\",  # 4-bit weights, 16-bit activations\n",
    "    ignore=[\"lm_head\"],  # Don't quantize the output layer\n",
    ")\n",
    "\n",
    "print(\"Quantization config:\")\n",
    "print(\"  - Scheme: W4A16 (4-bit weights)\")\n",
    "print(\"  - Target: Linear layers\")\n",
    "print(\"  - Ignored: lm_head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Starting Quantization (15-30 mins)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "oneshot(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=tokenized_dataset,\n",
    "    recipe=recipe,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_calibration_samples=len(calibration_texts),\n",
    "    save_compressed=True,\n",
    ")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Quantization Complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save Processor and Config Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Save tokenizer and processor\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved tokenizer and processor\")\n",
    "\n",
    "# Copy extra config files\n",
    "for f in [\"preprocessor_config.json\", \"generation_config.json\", \"chat_template.jinja\",\n",
    "          \"modeling_dots.py\", \"modeling_dots_vision.py\", \"configuration_dots.py\",\n",
    "          \"image_processing_dots.py\", \"processing_dots.py\"]:\n",
    "    src = os.path.join(MODEL_DIR, f)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, os.path.join(OUTPUT_DIR, f))\n",
    "        print(f\"Copied: {f}\")\n",
    "\n",
    "print(f\"\\nSaved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show output files\n",
    "total = 0\n",
    "print(\"\\nQuantized model files:\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    path = os.path.join(OUTPUT_DIR, f)\n",
    "    if os.path.isfile(path):\n",
    "        size = os.path.getsize(path) / 1024**2\n",
    "        total += size\n",
    "        print(f\"  {f}: {size:.1f} MB\")\n",
    "print(f\"\\nTotal: {total:.1f} MB ({total/1024:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verify Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Loading quantized model...\")\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "print(f\"Loaded! GPU mem: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test\n",
    "test_input = tokenizer(\"What text is in this image?\", return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    output = quantized_model.generate(**test_input, max_new_tokens=20, do_sample=False)\n",
    "print(f\"Output: {tokenizer.decode(output[0], skip_special_tokens=True)}\")\n",
    "print(\"\\n‚úì Quantized model working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create ZIP for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /kaggle/working && zip -r dots_ocr_awq_4bit.zip dots_ocr_awq_4bit/\n",
    "\n",
    "zip_size = os.path.getsize(\"/kaggle/working/dots_ocr_awq_4bit.zip\") / 1024**3\n",
    "print(f\"\\n‚úì Created: dots_ocr_awq_4bit.zip ({zip_size:.2f} GB)\")\n",
    "print(\"\\nDownload from Output tab ‚Üí\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done! üéâ\n",
    "\n",
    "**Usage:**\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"dots_ocr_awq_4bit\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dots_ocr_awq_4bit\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"dots_ocr_awq_4bit\", trust_remote_code=True)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
