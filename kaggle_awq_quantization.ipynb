{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWQ Quantization for dots.ocr Model\n",
    "\n",
    "Using **llm-compressor** (vLLM's official successor to AutoAWQ)\n",
    "\n",
    "**‚ö†Ô∏è BEFORE RUNNING - Verify GPU is enabled:**\n",
    "1. Right panel ‚Üí Settings ‚Üí Accelerator ‚Üí **GPU T4 x2**\n",
    "2. Right panel ‚Üí Settings ‚Üí Internet ‚Üí **On**\n",
    "3. You should see \"GPU T4 x2\" in the top right corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Verify GPU is Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this FIRST to verify GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"\\n‚úì GPU is ready!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå NO GPU DETECTED!\")\n",
    "    print(\"\\nTo fix:\")\n",
    "    print(\"1. Go to Settings (right panel)\")\n",
    "    print(\"2. Under 'Accelerator', select 'GPU T4 x2'\")\n",
    "    print(\"3. Wait for session to restart\")\n",
    "    print(\"4. Re-run this cell\")\n",
    "    raise RuntimeError(\"GPU not available. Enable it in Kaggle settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llmcompressor --quiet\n",
    "!pip install accelerate --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "MODEL_ID = \"rednote-hilab/dots.ocr\"\n",
    "MODEL_DIR = \"/kaggle/working/dots_ocr_original\"\n",
    "OUTPUT_DIR = \"/kaggle/working/dots_ocr_awq_4bit\"\n",
    "\n",
    "print(f\"Downloading {MODEL_ID}...\")\n",
    "snapshot_download(repo_id=MODEL_ID, local_dir=MODEL_DIR, local_dir_use_symlinks=False)\n",
    "print(f\"Downloaded to {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show files\n",
    "for f in os.listdir(MODEL_DIR):\n",
    "    path = os.path.join(MODEL_DIR, f)\n",
    "    if os.path.isfile(path):\n",
    "        print(f\"{f}: {os.path.getsize(path)/1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5: Patch DotsVLProcessor for Newer Transformers\n",
    "\n",
    "The DotsVLProcessor class needs to be patched to work with newer transformers that require video_processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch the DotsVLProcessor class to handle video_processor requirement\n",
    "import re\n",
    "\n",
    "config_path = os.path.join(MODEL_DIR, \"configuration_dots.py\")\n",
    "with open(config_path, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Replace the DotsVLProcessor class definition\n",
    "old_class_pattern = r'class DotsVLProcessor\\(Qwen2_5_VLProcessor\\):.*?(?=\\nclass |\\Z)'\n",
    "new_class = '''class DotsVLProcessor(Qwen2_5_VLProcessor):\n",
    "    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n",
    "        # If video_processor is None, create one to satisfy newer transformers requirement\n",
    "        if video_processor is None:\n",
    "            try:\n",
    "                from transformers import Qwen2VLVideoProcessor\n",
    "                video_processor = Qwen2VLVideoProcessor()\n",
    "            except (ImportError, Exception):\n",
    "                # Create a minimal video processor if the import fails\n",
    "                from transformers.models.qwen2_vl.video_processing_qwen2_vl import Qwen2VLVideoProcessor\n",
    "                video_processor = Qwen2VLVideoProcessor()\n",
    "        \n",
    "        super().__init__(image_processor, tokenizer, video_processor=video_processor, chat_template=chat_template, **kwargs)\n",
    "        self.image_token = \"<|imgpad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n",
    "        self.image_token_id = 151665 if not hasattr(tokenizer, \"image_token_id\") else tokenizer.image_token_id\n",
    "'''\n",
    "\n",
    "new_content = re.sub(old_class_pattern, new_class, content, flags=re.DOTALL)\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(new_content)\n",
    "\n",
    "print(\"‚úì Patched DotsVLProcessor to handle video_processor requirement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Model, Tokenizer, and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, AutoImageProcessor\n",
    "import gc\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor with manual video_processor construction\n",
    "print(\"Loading processor...\")\n",
    "\n",
    "# Clear any cached modules to pick up the patched version\n",
    "import sys\n",
    "modules_to_remove = [key for key in sys.modules.keys() if 'dots_ocr_original' in key]\n",
    "for mod in modules_to_remove:\n",
    "    del sys.modules[mod]\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
    "print(f\"Processor loaded: {type(processor).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model on GPU\n",
    "print(\"Loading model to GPU...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",  # Explicitly use CUDA\n",
    ")\n",
    "print(f\"Model loaded! GPU mem: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Calibration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Calibration prompts\n",
    "calibration_texts = [\n",
    "    \"What text is shown in this image?\",\n",
    "    \"Please read and transcribe the text from this document.\",\n",
    "    \"Extract all the text content from the provided image.\",\n",
    "    \"Can you identify and read the text in this picture?\",\n",
    "    \"Transcribe the handwritten text in this image.\",\n",
    "    \"What does this document say?\",\n",
    "    \"Read the printed text from this image.\",\n",
    "    \"Please OCR this image and provide the text.\",\n",
    "    \"Extract text content including numbers and special characters.\",\n",
    "    \"Identify all readable text elements in this image.\",\n",
    "    \"What is written on this page?\",\n",
    "    \"Transcribe the following scanned document.\",\n",
    "    \"Please read the text from this screenshot.\",\n",
    "    \"Extract and format the text content visible in the image.\",\n",
    "    \"Read all text including headers and footnotes.\",\n",
    "    \"What are the words shown in this picture?\",\n",
    "    \"Describe what you see in the image.\",\n",
    "    \"List all words visible in this photograph.\",\n",
    "    \"Convert the printed text to digital format.\",\n",
    "    \"Read and output the document content.\",\n",
    "    \"Extract text from this scanned page.\",\n",
    "    \"What words appear in this image?\",\n",
    "    \"Transcribe the visible text accurately.\",\n",
    "    \"Identify all text elements in the picture.\",\n",
    "    \"Read the content of this document image.\",\n",
    "    \"Extract and list all readable text.\",\n",
    "    \"What is the text content of this image?\",\n",
    "    \"Please digitize the text in this scan.\",\n",
    "    \"Convert image text to editable format.\",\n",
    "    \"Read aloud what this document says.\",\n",
    "    \"Extract every word from this image.\",\n",
    "    \"What textual information is present?\"\n",
    "]\n",
    "\n",
    "calibration_dataset = Dataset.from_dict({\"text\": calibration_texts})\n",
    "print(f\"Calibration dataset: {len(calibration_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize calibration data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = calibration_dataset.map(tokenize_function, batched=True)\n",
    "print(\"Calibration data tokenized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Quantize with AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor.modifiers.quantization import QuantizationModifier\n",
    "from llmcompressor import oneshot\n",
    "\n",
    "# AWQ-style quantization recipe\n",
    "recipe = QuantizationModifier(\n",
    "    targets=\"Linear\",\n",
    "    scheme=\"W4A16\",  # 4-bit weights, 16-bit activations\n",
    "    ignore=[\"lm_head\"],  # Don't quantize the output layer\n",
    ")\n",
    "\n",
    "print(\"Quantization config:\")\n",
    "print(\"  - Scheme: W4A16 (4-bit weights)\")\n",
    "print(\"  - Target: Linear layers\")\n",
    "print(\"  - Ignored: lm_head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Starting Quantization (15-30 mins)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "oneshot(\n",
    "    model=model,\n",
    "    processor=processor,  # Processor includes tokenizer - don't pass both!\n",
    "    dataset=tokenized_dataset,\n",
    "    recipe=recipe,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_calibration_samples=len(calibration_texts),\n",
    "    save_compressed=True,\n",
    ")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Quantization Complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save Processor and Config Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Save tokenizer and processor\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved tokenizer and processor\")\n",
    "\n",
    "# Copy extra config files (including the patched configuration_dots.py)\n",
    "for f in [\"preprocessor_config.json\", \"generation_config.json\", \"chat_template.jinja\",\n",
    "          \"modeling_dots.py\", \"modeling_dots_vision.py\", \"configuration_dots.py\",\n",
    "          \"image_processing_dots.py\", \"processing_dots.py\"]:\n",
    "    src = os.path.join(MODEL_DIR, f)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, os.path.join(OUTPUT_DIR, f))\n",
    "        print(f\"Copied: {f}\")\n",
    "\n",
    "print(f\"\\nSaved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show output files\n",
    "total = 0\n",
    "print(\"\\nQuantized model files:\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    path = os.path.join(OUTPUT_DIR, f)\n",
    "    if os.path.isfile(path):\n",
    "        size = os.path.getsize(path) / 1024**2\n",
    "        total += size\n",
    "        print(f\"  {f}: {size:.1f} MB\")\n",
    "print(f\"\\nTotal: {total:.1f} MB ({total/1024:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verify Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Loading quantized model...\")\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "print(f\"Loaded! GPU mem: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test\n",
    "test_input = tokenizer(\"What text is in this image?\", return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    output = quantized_model.generate(**test_input, max_new_tokens=20, do_sample=False)\n",
    "print(f\"Output: {tokenizer.decode(output[0], skip_special_tokens=True)}\")\n",
    "print(\"\\n‚úì Quantized model working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create ZIP for Download (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /kaggle/working && zip -r dots_ocr_awq_4bit.zip dots_ocr_awq_4bit/\n",
    "\n",
    "zip_size = os.path.getsize(\"/kaggle/working/dots_ocr_awq_4bit.zip\") / 1024**3\n",
    "print(f\"\\n‚úì Created: dots_ocr_awq_4bit.zip ({zip_size:.2f} GB)\")\n",
    "print(\"\\nDownload from Output tab ‚Üí\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Push to HuggingFace Hub üöÄ\n",
    "\n",
    "Upload the quantized model to HuggingFace so you can use it directly with `transformers`.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create a HuggingFace account at https://huggingface.co\n",
    "2. Create an access token at https://huggingface.co/settings/tokens (with **write** permissions)\n",
    "3. Add your token as a Kaggle secret named `HF_TOKEN`:\n",
    "   - Go to Add-ons ‚Üí Secrets\n",
    "   - Add secret with Label: `HF_TOKEN`, Value: your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Get HuggingFace token from Kaggle secrets\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "    print(\"‚úì HuggingFace token loaded from Kaggle secrets\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Could not load HF_TOKEN from Kaggle secrets\")\n",
    "    print(\"Please add your HuggingFace token as a Kaggle secret named 'HF_TOKEN'\")\n",
    "    print(\"Go to: Add-ons ‚Üí Secrets ‚Üí Add secret\")\n",
    "    raise e\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(token=hf_token)\n",
    "print(\"‚úì Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è CHANGE THIS to your HuggingFace username/repo-name\n",
    "HF_REPO_ID = \"sugam24/dots-ocr-awq-4bit\"  # e.g., \"sugam/dots-ocr-awq-4bit\"\n",
    "\n",
    "print(f\"Will upload to: https://huggingface.co/{HF_REPO_ID}\")\n",
    "print(\"\\n‚ö†Ô∏è Make sure to change HF_REPO_ID above to your username!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a README for the model\n",
    "readme_content = f\"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: rednote-hilab/dots.ocr\n",
    "tags:\n",
    "  - ocr\n",
    "  - vision\n",
    "  - quantized\n",
    "  - awq\n",
    "  - 4bit\n",
    "library_name: transformers\n",
    "pipeline_tag: image-to-text\n",
    "---\n",
    "\n",
    "# dots.ocr AWQ 4-bit Quantized\n",
    "\n",
    "This is a 4-bit AWQ quantized version of [rednote-hilab/dots.ocr](https://huggingface.co/rednote-hilab/dots.ocr).\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model**: rednote-hilab/dots.ocr\n",
    "- **Quantization**: W4A16 (4-bit weights, 16-bit activations)\n",
    "- **Method**: llm-compressor\n",
    "- **Size**: ~1.5GB (reduced from ~6GB)\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"{HF_REPO_ID}\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{HF_REPO_ID}\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"{HF_REPO_ID}\", trust_remote_code=True)\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "Same as the base model (Apache 2.0).\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"README.md\"), \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"‚úì Created README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to HuggingFace Hub\n",
    "api = HfApi()\n",
    "\n",
    "print(f\"Uploading to {HF_REPO_ID}...\")\n",
    "print(\"This may take 5-15 minutes depending on your connection.\")\n",
    "print()\n",
    "\n",
    "api.create_repo(repo_id=HF_REPO_ID, exist_ok=True, private=False)\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=OUTPUT_DIR,\n",
    "    repo_id=HF_REPO_ID,\n",
    "    commit_message=\"Upload AWQ 4-bit quantized dots.ocr model\",\n",
    ")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"‚úì Upload Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nModel available at: https://huggingface.co/{HF_REPO_ID}\")\n",
    "print(f\"\\nUsage:\")\n",
    "print(f'  model = AutoModelForCausalLM.from_pretrained(\"{HF_REPO_ID}\", trust_remote_code=True, device_map=\"cuda\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done! üéâ\n",
    "\n",
    "Your quantized model is now on HuggingFace! Use it anywhere with:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"YOUR_USERNAME/dots-ocr-awq-4bit\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"YOUR_USERNAME/dots-ocr-awq-4bit\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"YOUR_USERNAME/dots-ocr-awq-4bit\", trust_remote_code=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Inference with Uploaded Model üîç\n",
    "\n",
    "Now let's test our uploaded quantized model by running OCR on a real image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for inference\n",
    "print(\"Installing compressed-tensors for quantized model loading...\")\n",
    "!pip install compressed-tensors --quiet\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")\n",
    "print(\"‚ö†Ô∏è  IMPORTANT: You may need to restart the kernel after installation\")\n",
    "print(\"   If you get import errors in the next cell, restart the kernel and re-run from this cell\")\n",
    "\n",
    "# Test if the import works\n",
    "try:\n",
    "    import compressed_tensors\n",
    "    print(\"‚úì compressed-tensors successfully imported!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"üí° Please restart the kernel and re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Inference Setup Instructions\n",
    "\n",
    "**IMPORTANT: Follow these steps in order:**\n",
    "\n",
    "1. **Run the installation cell below** (to install compressed-tensors)\n",
    "2. **If you get import errors later**, restart the kernel:\n",
    "   - Kernel ‚Üí Restart Kernel \n",
    "   - Then re-run from the installation cell\n",
    "3. **Run the model loading cell** \n",
    "4. **Continue with image loading and inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Clear previous model from memory\n",
    "try:\n",
    "    del quantized_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úì Cleared previous model from memory\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Load the quantized model from HuggingFace\n",
    "print(\"Loading quantized model from HuggingFace...\")\n",
    "model_inference = AutoModelForCausalLM.from_pretrained(\n",
    "    \"sugam24/dots-ocr-awq-4bit\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"eager\"  # Use eager attention to avoid mixed precision issues\n",
    ")\n",
    "\n",
    "# Note: Quantized models are already in the correct dtype, no need to call .half()\n",
    "\n",
    "tokenizer_inference = AutoTokenizer.from_pretrained(\"sugam24/dots-ocr-awq-4bit\", trust_remote_code=True)\n",
    "processor_inference = AutoProcessor.from_pretrained(\"sugam24/dots-ocr-awq-4bit\", trust_remote_code=True)\n",
    "\n",
    "print(f\"‚úì Model loaded from HuggingFace! GPU mem: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_path = \"/kaggle/input/images/doc1.jpeg\"\n",
    "\n",
    "# Check if the image exists\n",
    "if os.path.exists(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    print(f\"‚úì Image loaded: {image.size} ({image.mode})\")\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Input Image for OCR\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"‚ùå Image not found at: {image_path}\")\n",
    "    print(\"Available files in /kaggle/input/:\")\n",
    "    for root, dirs, files in os.walk(\"/kaggle/input/\"):\n",
    "        for file in files:\n",
    "            print(f\"  {os.path.join(root, file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OCR inference\n",
    "print(\"Running OCR inference...\")\n",
    "\n",
    "# Prepare the input - dots.ocr uses a simpler approach\n",
    "prompt = \"What text is shown in this image?\"\n",
    "\n",
    "# Process image and text together using the dots.ocr processor\n",
    "inputs = processor_inference(\n",
    "    text=prompt,\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Ensure all inputs are float16 to match the model (more comprehensive)\n",
    "print(\"Converting input dtypes...\")\n",
    "for key in inputs:\n",
    "    if isinstance(inputs[key], torch.Tensor):\n",
    "        print(f\"  {key}: {inputs[key].dtype} -> float16\")\n",
    "        # Force conversion to float16 regardless of current dtype\n",
    "        inputs[key] = inputs[key].to(torch.float16)\n",
    "    inputs[key] = inputs[key].to(\"cuda\")\n",
    "\n",
    "print(f\"‚úì Input processed. Keys: {list(inputs.keys())}\")\n",
    "\n",
    "# Generate the response with explicit dtype management\n",
    "print(\"Generating response...\")\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):  # Force float16 autocast\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model_inference.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer_inference.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "# Decode the response\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "output_text = processor_inference.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "print(\"‚úì OCR inference completed!\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXTRACTED TEXT:\")\n",
    "print(\"=\"*50)\n",
    "print(output_text)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to markdown file\n",
    "from datetime import datetime\n",
    "\n",
    "# Create markdown content\n",
    "markdown_content = f\"\"\"# OCR Results - dots.ocr AWQ 4-bit\n",
    "\n",
    "## Inference Details\n",
    "- **Model**: sugam24/dots-ocr-awq-4bit\n",
    "- **Quantization**: W4A16 (4-bit weights, 16-bit activations)\n",
    "- **Input Image**: {image_path}\n",
    "- **Image Size**: {image.size}\n",
    "- **Timestamp**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Prompt\n",
    "```\n",
    "{prompt}\n",
    "```\n",
    "\n",
    "## Extracted Text\n",
    "\n",
    "```\n",
    "{output_text}\n",
    "```\n",
    "\n",
    "---\n",
    "*Generated using the AWQ 4-bit quantized version of dots.ocr model*\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "output_md_path = \"/kaggle/working/ocr_results.md\"\n",
    "with open(output_md_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(markdown_content)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {output_md_path}\")\n",
    "print(f\"üìÑ File size: {os.path.getsize(output_md_path)} bytes\")\n",
    "\n",
    "# Also display the markdown content\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MARKDOWN CONTENT:\")\n",
    "print(\"=\"*50)\n",
    "print(markdown_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and summary\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"üéØ INFERENCE COMPLETE!\")\n",
    "print(\"\\nüìã Summary:\")\n",
    "print(f\"   ‚Ä¢ Model: sugam24/dots-ocr-awq-4bit\")\n",
    "print(f\"   ‚Ä¢ Image: {image_path}\")\n",
    "print(f\"   ‚Ä¢ Results: {output_md_path}\")\n",
    "print(f\"   ‚Ä¢ GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\nüìÅ Available outputs:\")\n",
    "print(\"   ‚Ä¢ OCR results: /kaggle/working/ocr_results.md\")\n",
    "print(\"   ‚Ä¢ Quantized model: /kaggle/working/dots_ocr_awq_4bit/\")\n",
    "print(\"   ‚Ä¢ Model ZIP: /kaggle/working/dots_ocr_awq_4bit.zip\")\n",
    "\n",
    "print(\"\\n‚ú® The quantized model is now ready for production use!\")\n",
    "print(\"   You can download the markdown file from the Output tab.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
