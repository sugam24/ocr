{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dots.ocr Inference Notebook\n",
        "\n",
        "Run OCR on your documents using the quantized dots.ocr model.\n",
        "\n",
        "**Setup:**\n",
        "1. Enable GPU: Settings → Accelerator → **GPU T4 x2**\n",
        "2. Enable Internet: Settings → Internet → **On**\n",
        "3. Upload your image in the left panel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies (Kaggle)\n",
        "# NOTE: Kaggle often ships a torchvision build that is incompatible with Pillow 12+.\n",
        "# Pin Pillow < 12 and Transformers < 5 to avoid common import conflicts.\n",
        "# After this cell finishes, use Kaggle: \"Restart session\" (top right) and rerun from the top.\n",
        "!pip -q install -U --force-reinstall \\\n",
        "  \"pillow<12\" \\\n",
        "  \"transformers>=4.46.0,<5\" \\\n",
        "  \"accelerate>=0.26.0,<1\" \\\n",
        "  \"pdf2image\" \\\n",
        "  \"qwen-vl-utils\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Optional) sanity check versions\n",
        "import PIL\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "print(\"python:\", __import__(\"sys\").version)\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"pillow:\", PIL.__version__)\n",
        "\n",
        "# If you still see a Pillow/torchvision import error, restart the session and rerun from the top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoImageProcessor, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Your quantized model\n",
        "MODEL_ID = \"sugam24/dots-ocr-awq-4bit\"\n",
        "\n",
        "print(f\"Loading {MODEL_ID}...\")\n",
        "\n",
        "# IMPORTANT:\n",
        "# - `device_map=\"cuda\"` is NOT valid in Transformers. Use \"auto\".\n",
        "# - `AutoProcessor` can fail on some setups for this model (video-processor auto-detection).\n",
        "#   We avoid it by using `tokenizer` + `AutoImageProcessor` directly.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "image_processor = AutoImageProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"✓ Model loaded! GPU mem: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Your Image\n",
        "\n",
        "Upload your image using the file browser on the left, or run the cell below to upload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kaggle: put your file in either:\n",
        "# - /kaggle/input/<your-dataset>/...\n",
        "# - /kaggle/working/...\n",
        "# Then set IMAGE_PATH below.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import display, Image as IPImage\n",
        "\n",
        "# Option A (recommended): set this manually\n",
        "# IMAGE_PATH = \"/kaggle/input/your-dataset/image.jpg\"\n",
        "# IMAGE_PATH = \"/kaggle/input/your-dataset/document.pdf\"\n",
        "\n",
        "# Option B: auto-pick the first image/pdf found under /kaggle/input or /kaggle/working\n",
        "candidates = []\n",
        "for root in (\"/kaggle/input\", \"/kaggle/working\"):\n",
        "    p = Path(root)\n",
        "    if p.exists():\n",
        "        for ext in (\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.webp\", \"*.tif\", \"*.tiff\", \"*.pdf\"):\n",
        "            candidates.extend(list(p.rglob(ext)))\n",
        "\n",
        "if not candidates:\n",
        "    raise FileNotFoundError(\n",
        "        \"No input files found. Add a dataset (left panel) or upload a file into /kaggle/working, then set IMAGE_PATH.\"\n",
        "    )\n",
        "\n",
        "IMAGE_PATH = str(candidates[0])\n",
        "print(f\"✓ Using: {IMAGE_PATH}\")\n",
        "\n",
        "# Display if it's an image\n",
        "if Path(IMAGE_PATH).suffix.lower() != \".pdf\":\n",
        "    display(IPImage(filename=IMAGE_PATH, width=700))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 2: Specify path directly (if you uploaded via file browser)\n",
        "# IMAGE_PATH = \"/kaggle/input/your-dataset/image.png\"\n",
        "# or\n",
        "# IMAGE_PATH = \"/kaggle/working/uploaded_image.png\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Run OCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def load_image_or_pdf(path: str) -> Image.Image:\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(path)\n",
        "\n",
        "    if p.suffix.lower() == \".pdf\":\n",
        "        # First page only (you can extend this later)\n",
        "        from pdf2image import convert_from_path\n",
        "\n",
        "        pages = convert_from_path(str(p), first_page=1, last_page=1)\n",
        "        return pages[0].convert(\"RGB\")\n",
        "\n",
        "    return Image.open(str(p)).convert(\"RGB\")\n",
        "\n",
        "\n",
        "def run_ocr(image_path: str, prompt: str = \"Extract all the text from this image.\", max_new_tokens: int = 1024) -> str:\n",
        "    \"\"\"Run OCR on an image/PDF and return extracted text.\"\"\"\n",
        "    image = load_image_or_pdf(image_path)\n",
        "    print(f\"Image size: {image.size}\")\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Build model inputs without `AutoProcessor` (more reliable across environments)\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    text_inputs = tokenizer([text], padding=True, return_tensors=\"pt\")\n",
        "    image_inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    model_dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
        "\n",
        "    inputs = {**text_inputs, **image_inputs}\n",
        "    for k, v in list(inputs.items()):\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            v = v.to(device)\n",
        "            if torch.is_floating_point(v):\n",
        "                v = v.to(model_dtype)\n",
        "            inputs[k] = v\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if device.type == \"cuda\":\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=model_dtype):\n",
        "                output_ids = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=tokenizer.pad_token_id,\n",
        "                )\n",
        "        else:\n",
        "            output_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "    generated_ids = output_ids[0][inputs[\"input_ids\"].shape[1] :]\n",
        "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run OCR on your image\n",
        "print(\"Running OCR...\")\n",
        "result = run_ocr(IMAGE_PATH)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EXTRACTED TEXT:\")\n",
        "print(\"=\"*50)\n",
        "print(result)\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Try Different Prompts (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try a custom prompt\n",
        "custom_prompt = \"Extract all text including headers, paragraphs, and any tables from this document.\"\n",
        "\n",
        "result = run_ocr(IMAGE_PATH, prompt=custom_prompt)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process multiple images\n",
        "import glob\n",
        "\n",
        "# Find all images in a folder\n",
        "image_files = glob.glob(\"/kaggle/working/*.png\") + glob.glob(\"/kaggle/working/*.jpg\")\n",
        "\n",
        "for img_path in image_files:\n",
        "    print(f\"\\n--- Processing: {img_path} ---\")\n",
        "    result = run_ocr(img_path)\n",
        "    print(result[:500] + \"...\" if len(result) > 500 else result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save OCR result to Markdown (Kaggle output)\n",
        "from datetime import datetime\n",
        "\n",
        "output_file = \"/kaggle/working/ocr_result.md\"\n",
        "\n",
        "markdown_content = f\"\"\"# OCR Results - dots.ocr AWQ 4-bit\n",
        "\n",
        "## Inference Details\n",
        "- **Model**: {MODEL_ID}\n",
        "- **Input**: {IMAGE_PATH}\n",
        "- **Timestamp**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## Prompt\n",
        "```text\n",
        "{custom_prompt if 'custom_prompt' in globals() else 'Extract all the text from this image.'}\n",
        "```\n",
        "\n",
        "## Extracted Text\n",
        "```text\n",
        "{result}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(markdown_content)\n",
        "\n",
        "print(f\"✓ Saved to {output_file}\")\n",
        "print(\"You can download it from the Kaggle 'Output' pane.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
